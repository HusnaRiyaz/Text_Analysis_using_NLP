{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyAo1F7YyK4jIWuHn3OM+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HusnaRiyaz/Text_Analysis_using_NLP/blob/main/Black_coffer_NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iStdFwBXCvlR",
        "outputId": "4437e263-a2b9-477d-cea1-480cf6f2422f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract title and text from a URL\n",
        "def extract_title_and_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        title = soup.find('title').get_text(strip=True)\n",
        "        main_content = soup.find('div', class_='td-post-content')\n",
        "        article_content = main_content.find_all(['p', 'li'])\n",
        "        content = [element.get_text(separator=' ', strip=True) for element in article_content]\n",
        "        text = '\\n'.join(content)\n",
        "        return title, text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract data from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to read URLs from Excel file\n",
        "def read_urls_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    return df\n",
        "\n",
        "# Function to save text to a file\n",
        "def save_text_to_file(file_name, title, text):\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(title + \"\\n\\n\" + text)\n",
        "\n",
        "# Function to read stopwords from files\n",
        "def read_stopwords(folder):\n",
        "    stopwords_set = set()\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    for line in file:\n",
        "                        stopwords_set.add(line.strip().lower())\n",
        "            except UnicodeDecodeError:\n",
        "                with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
        "                    for line in file:\n",
        "                        stopwords_set.add(line.strip().lower())\n",
        "    return stopwords_set\n",
        "\n",
        "# Function to create a dictionary from the masterdict folder\n",
        "def create_word_dict(masterdict_folder, stopwords):\n",
        "    word_dict = {'positive': set(), 'negative': set()}\n",
        "    for filename in os.listdir(masterdict_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            filepath = os.path.join(masterdict_folder, filename)\n",
        "            category = 'positive' if 'positive' in filename.lower() else 'negative'\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    for line in file:\n",
        "                        word = line.strip().lower()\n",
        "                        if word not in stopwords:\n",
        "                            word_dict[category].add(word)\n",
        "            except UnicodeDecodeError:\n",
        "                with open(filepath, 'r', encoding='latin1', errors='ignore') as file:\n",
        "                    for line in file:\n",
        "                        word = line.strip().lower()\n",
        "                        if word not in stopwords:\n",
        "                            word_dict[category].add(word)\n",
        "    return word_dict\n",
        "\n",
        "# Function to calculate text analysis scores\n",
        "def calculate_scores(text, positive_words, negative_words):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
        "    negative_score = (sum(-1 for word in tokens if word in negative_words)*-1)\n",
        "    total_words = len(tokens)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Function to calculate readability metrics\n",
        "def calculate_readability_metrics(text, stopwords_set):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation for word count\n",
        "    clean_words = [word for word in words if word.isalnum() and word.lower() not in stopwords_set]\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(clean_words)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    # Calculate complex words (words with more than 2 syllables)\n",
        "    def count_syllables(word):\n",
        "        word = word.lower()\n",
        "        vowels = \"aeiouy\"\n",
        "        count = 0\n",
        "        if word[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word)):\n",
        "            if word[index] in vowels and word[index - 1] not in vowels:\n",
        "                count += 1\n",
        "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "        return count\n",
        "\n",
        "    complex_words = [word for word in clean_words if count_syllables(word) > 2]\n",
        "    num_complex_words = len(complex_words)\n",
        "\n",
        "    # Calculate percentage of complex words\n",
        "    percentage_complex_words = num_complex_words / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Calculate Fog Index\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate average word length\n",
        "    avg_word_length = sum(len(word) for word in clean_words) / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Calculate average number of words per sentence\n",
        "    avg_words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    # Calculate syllable count per word\n",
        "    total_syllables = sum(count_syllables(word) for word in clean_words)\n",
        "    syllable_count_per_word = total_syllables / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Count personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)) #search is case-insensitive\n",
        "\n",
        "    return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, num_complex_words, num_words, syllable_count_per_word, personal_pronouns, avg_word_length\n",
        "\n",
        "# Define the path to the URLs Excel file in Google Drive\n",
        "excel_file_path = '/content/drive/My Drive/Black_coffer/Input.xlsx'\n",
        "\n",
        "# Define the path to the stopwords folder in Google Drive\n",
        "stopwords_folder = '/content/drive/My Drive/Black_coffer/Stop_words'\n",
        "\n",
        "# Define the path to the masterdict folder in Google Drive\n",
        "masterdict_folder = '/content/drive/My Drive/Black_coffer/MasterDictionary'\n",
        "\n",
        "# Define the output folder in Google Drive\n",
        "output_folder = '/content/drive/My Drive/Black_coffer/extracted_texts'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Read stopwords\n",
        "stopwords_set = read_stopwords(stopwords_folder)\n",
        "\n",
        "# Create the word dictionary\n",
        "word_dict = create_word_dict(masterdict_folder, stopwords_set)\n",
        "\n",
        "# Read URLs from Excel file\n",
        "df = read_urls_from_excel(excel_file_path)\n",
        "\n",
        "# Prepare the output DataFrame\n",
        "output_data = []\n",
        "\n",
        "# Process each URL\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, text = extract_title_and_text(url)\n",
        "    if title and text:\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score = calculate_scores(\n",
        "            text, word_dict['positive'], word_dict['negative'])\n",
        "        avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, num_complex_words, num_words, syllable_count_per_word, personal_pronouns, avg_word_length = calculate_readability_metrics(text, stopwords_set)\n",
        "\n",
        "        output_data.append({\n",
        "            'URL_ID': url_id,\n",
        "            'URL': url,\n",
        "            'POSITIVE SCORE': positive_score,\n",
        "            'NEGATIVE SCORE': negative_score,\n",
        "            'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "            'FOG INDEX': fog_index,\n",
        "            'AVG WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "            'COMPLEX WORD COUNT': num_complex_words,\n",
        "            'WORD COUNT': num_words,\n",
        "            'SYLLABLE PER WORD': syllable_count_per_word,\n",
        "            'PERSONAL PRONOUNS': personal_pronouns,\n",
        "            'AVG WORD LENGTH': avg_word_length\n",
        "        })\n",
        "\n",
        "        file_name = os.path.join(output_folder, f\"{url_id}.txt\")\n",
        "        save_text_to_file(file_name, title, text)\n",
        "\n",
        "# Create DataFrame and save to Excel\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_excel('/content/drive/My Drive/Black_coffer/Output_text_analysis.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AceqFxpLcqIm",
        "outputId": "30aa0390-7d90-47d2-e8c7-f7e263bb9b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to extract data from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 'NoneType' object has no attribute 'find_all'\n",
            "Failed to extract data from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 'NoneType' object has no attribute 'find_all'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **URL - 36, 49 returns 404 error so failed to extract data**"
      ],
      "metadata": {
        "id": "DCHfNnGIgWIW"
      }
    }
  ]
}